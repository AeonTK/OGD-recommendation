"""Ingest dataset description column into Milvus vector store.

Relies on internal vectorstore components under src/vectorstore/:
	- Embedder (embeddings)
	- DataVectorStore (data_store)

Environment:
	COHERE_API_KEY  (required for embedding)
	MILVUS_URI      (default: http://localhost:19530)
	MILVUS_TOKEN    (default: root:Milvus)

Notes:
	The DataVectorStore schema supports hybrid search (dense + BM25). We only populate
	dense vectors here; sparse vectors are generated by Milvus BM25 function on ingestion.

Configuration is done via module-level constants below (no CLI arguments).
"""

from __future__ import annotations

import sys
import uuid
import logging
from pathlib import Path
from typing import List

import pandas as pd

# Ensure 'src' is on sys.path so we can import vectorstore package
CURRENT_DIR = Path(__file__).resolve().parent
SRC_DIR = CURRENT_DIR.parent / "src"
if str(SRC_DIR) not in sys.path:
	sys.path.insert(0, str(SRC_DIR))

from vectorstore.data_store import DataVectorStore  # type: ignore  # noqa: E402

# ---------------------------------------------------------------------------
# Configuration Constants (modify as needed)
# ---------------------------------------------------------------------------
CSV_PATH: str = "dataset-500.csv"
TEXT_COLUMN: str = "description.en"
COLLECTION_NAME: str = "programs"
RECREATE_COLLECTION: bool = False  # Set True to drop & recreate collection
BATCH_SIZE: int = 512
ROW_LIMIT: int = 0  # 0 means ingest all rows
LOG_LEVEL: str = "INFO"  # One of: CRITICAL, ERROR, WARNING, INFO, DEBUG

# NOTE: Query/search functionality has been moved to scripts/search.py


# (Argument parsing removed per request; constants above control behavior.)


def load_dataframe(csv_path: str, column: str) -> pd.DataFrame:
	df = pd.read_csv(csv_path)
	if column not in df.columns:
		raise ValueError(
			f"Column '{column}' not in CSV. Available columns: {list(df.columns)[:10]}... (total {len(df.columns)})"
		)
	return df


def select_ids(df: pd.DataFrame) -> List[str]:
	"""Generate fresh UUIDv4 strings for every row.

	This intentionally ignores any existing 'id' column in the CSV to ensure
	compliance with the Milvus schema (VARCHAR(36)). Original IDs (if needed)
	could be stored as an additional field in the schema in the future.
	"""
	return [str(uuid.uuid4()) for _ in range(len(df))]


def clean_texts(series: pd.Series) -> List[str]:
	texts: List[str] = []
	for val in series.fillna("").astype(str).tolist():
		t = val.strip()
		if not t:
			continue
		texts.append(t)
	return texts


def ingest(csv_path: str, column: str, collection: str, recreate: bool, batch_size: int, limit: int) -> DataVectorStore:
	logger = logging.getLogger(__name__)
	store = DataVectorStore(collection=collection)
	if recreate:
		logger.info("Recreating collection '%s' ...", collection)
		store.reset()
	logger.info("Loading CSV: %s", csv_path)
	df = load_dataframe(csv_path, column)

	if limit and limit > 0:
		df = df.head(limit)
		logger.info("Limiting to first %d rows", len(df))

	ids_full = select_ids(df)
	texts_full = df[column].fillna("").astype(str).tolist()

	# Filter out empty texts while keeping id alignment
	filtered_ids: List[str] = []
	filtered_texts: List[str] = []
	for i, txt in enumerate(texts_full):
		st = txt.strip()
		if not st:
			continue
		filtered_ids.append(ids_full[i])
		filtered_texts.append(st)

	if not filtered_texts:
		raise ValueError("No non-empty texts to ingest.")

	embedder = store.embedder  # Reuse embedder to ensure dim consistency
	logger.info(
		"Embedding and upserting %d texts (batch size %d)...", len(filtered_texts), batch_size
	)

	total = 0
	for start in range(0, len(filtered_texts), batch_size):
		end = start + batch_size
		batch_ids = filtered_ids[start:end]
		batch_texts = filtered_texts[start:end]
		vectors = embedder.embed_documents(batch_texts)
		store.upsert(batch_ids, batch_texts, vectors)
		total += len(batch_texts)
		logger.debug("Batch upserted size=%d cumulative=%d", len(batch_texts), total)
	logger.info(
		"Completed ingestion: %d texts into collection '%s' (dim=%d).",
		total,
		collection,
		embedder.dim,
	)
	return store


# Query logic moved to scripts/search.py


def main() -> int:
	logging.basicConfig(
		level=getattr(logging, LOG_LEVEL.upper(), logging.INFO),
		format="%(asctime)s %(levelname)s %(name)s - %(message)s",
	)
	logger = logging.getLogger(__name__)
	try:
		ingest(
			csv_path=CSV_PATH,
			column=TEXT_COLUMN,
			collection=COLLECTION_NAME,
			recreate=RECREATE_COLLECTION,
			batch_size=BATCH_SIZE,
			limit=ROW_LIMIT,
		)
		logger.info("Ingestion finished. Use scripts/search.py to run queries.")
		return 0
	except Exception as e:  # pragma: no cover
		logger.exception("Error during ingestion: %s", e)
		return 1


if __name__ == "__main__":  # pragma: no cover
	raise SystemExit(main())
